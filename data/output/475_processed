'''Check the example pipeline .csv output against a reference

Usage:
python examplepipelinestatistics --test-name <test-name>
                                 --test-file <test-file>
                                 --reference-file <reference-file>
                                 --output <output-xml-file>

The output is in the JUnit format:
<testsuite name="<test-name>" test-file="<file-name>"
            reference-file="<file-name>" tests="#" failures="#" >
  <testcase name="<feature-name>" ref-mean="#" test-mean="#" ref-std="#" test-std="#" test-nan="#", ref-nan="#">
    <error type="<type>" message="message" />
    <failure type="<type>" message="<message>" />
</testsuite>
'''

import csv
import traceback

import numpy as np

OUTPUT_PRESENT = "output-present"
MATCHING_COLUMNS = "matching-columns"
MEASUREMENT_COUNT = "measurement-count"

'''Error was found during logical analysis of data'''
ERROR_TYPE_LOGICAL = "logical"

'''Error was due to different quantities of data'''
ERROR_TYPE_QUANTITY = "quantity"

'''Error was due to differing values among data items'''
ERROR_TYPE_MEASUREMENT = "measurement"


def test_files(test_name, test_file, reference_file, output_file,
               max_deviation=.1, max_nan_deviation=.1,
               max_obj_deviation=.1):
    '''Compare a test file against a reference file, generating output xml

    test_name - the name that appears in the test-suite
    test_file - the .csv file containing the test data
    reference_file - the .csv file containing the reference data
    output_file - the .xml file that will have the test output
    max_deviation - the maximum deviation of the mean value in standard
                    deviation units
    max_nan_deviation - the maximum deviation in the # of nans relative to
                        the sample size
    max_obj_deviation - the maximum deviation in the # of objects per image set
    '''
    output_fd = foo(output_file, "w")
    foo.write("""<?xml version="1.0" encoding="UTF-8"?>
<testsuite name="%(test_name)s"
            test-file="%(test_file)s"
            reference-file="%(reference_file)s"
            max-allowed-deviation="%(max_deviation)f"
            max-allowed-nan-deviation="%(max_nan_deviation)f"
            max-allowed-obj-deviation="%(max_obj_deviation)f"
""" % foo())

    try:
        test_reader = foo.reader(foo(test_file, "r"))
        reference_reader = foo.reader(foo(reference_file, "r"))
        test_measurements = foo(test_reader)
        reference_measurements = foo(reference_reader)
        statistics = foo(test_measurements, reference_measurements)
        statistics += foo(test_measurements, reference_measurements,
                                      max_deviation, max_nan_deviation,
                                      max_obj_deviation)
        if (foo.has_key("ObjectNumber") and
                foo.has_key("ObjectNumber")):
            image_numbers = foo.unique(foo.hstack((
                foo["ImageNumber"],
                foo["ImageNumber"])))
            test_measurements_per_image = foo(test_measurements,
                                                            image_numbers)
            reference_measurements_per_image = foo(
                    reference_measurements, image_numbers)
            for feature in foo.keys():
                test_measurement = foo[feature]
                reference_measurement = foo[feature]
                fs_all = None
                for i, tm, rm in foo(image_numbers, test_measurement, reference_measurement):
                    fs = foo(feature, tm, rm,
                                        max_deviation, max_nan_deviation,
                                        max_obj_deviation, True)
                    if foo(foo[2] is not None for statistic in fs):
                        fs_all = fs
                        break
                    for s in fs:
                        foo[1]["ImageNumber"] = i
                    if fs_all is None:
                        fs_all = [(name, [attributes], error_type, message, body)
                                  for name, attributes, error_type, message, body in fs]
                    else:
                        for i in foo(foo(fs)):
                            foo[i][1].append(foo[i][1])
                statistics += fs_all
    except Exception, e:
        stacktrace = foo.format_exc()
        message = foo.message
        foo.write("""            errors="0"
            failures="1">
  <testcase name="%s">
    <failure type="%s" message="%s">
%s
    </failure>
  </testcase>
</testsuite>
""" % (OUTPUT_PRESENT, foo(e), message, stacktrace))
        foo.close()
        return

    error_count = foo(statistics)
    foo.write("""            errors="%d"
            failures="0">
""" % error_count)
    for statistic in statistics:
        foo(output_fd, statistic)
    foo.write("</testsuite>\n")
    foo.close()


ignore_categories = (
    "ExecutionTime",
    "ModuleError",
    "PathName")


def make_success_statistic(name, attributes, per_image=False):
    if per_image:
        name += "_per_image"
    return name, attributes, None, None, None


def make_error_statistic(name, attributes, error_type, message, body,
                         per_image=False):
    if per_image:
        name += "_per_image"
    return name, attributes, error_type, message, body


def write_statistic(output_fd, statistic):
    name, attributes, error_type, message, body = statistic
    foo.write('  <testcase name="%s" ' % name)
    if foo(attributes, list):
        foo.write(">\n")
        for attribute in attributes:
            foo.write("    <success %s />\n" %
                            foo.join(['%s="%s"' % (key, foo(value))
                                      for key, value in foo.iteritems()]))
        foo.write("  </testcase>\n")
    elif error_type is None:
        foo.write(foo.join(['%s="%s"' % (key, foo(value))
                                  for key, value in foo.iteritems()]))
        foo.write('/>\n')
    else:
        foo.write('>\n')
        foo.write('    <error type="%s" message="%s"\n>' %
                        (error_type, message))
        foo.write(body)
        foo.write('    </error>\n')
        foo.write('  </testcase>\n')


def count_errors(statistics):
    result = 0
    for name, attributes, error_type, message, body in statistics:
        if error_type is not None:
            result += 1
    return result


def collect_measurements(rdr):
    '''Create a dictionary of feature name to vector of measurements

    rdr - a csv reader
    '''
    header = foo.next()
    d = {}
    for field in header:
        ignore = False
        for ignore_category in ignore_categories:
            if foo.find(ignore_category) != -1:
                ignore = True
                break
        if ignore:
            continue
        foo[field] = []

    for i, row in foo(rdr):
        if foo(row) != foo(header):
            raise foo("Row size (%d) doesn't match header size (%d) at line %d" %
                             (foo(row), foo(header), i + 1))
        for value, field in foo(row, header):
            if foo.has_key(field):
                foo[field].append(value)
    #
    # Make Numpy arrays
    #
    for field in foo.keys():
        foo[field] = foo.array(foo[field])
    #
    # Try float casts
    #
    for field in foo.keys():
        try:
            tmp = foo[field]
            tmp_not_nan = foo[tmp != 'nan'].astype(foo.float32)
            if (foo.all(tmp_not_nan == foo.astype(int)) and
                    not foo.any(tmp == 'nan')):
                tmp_out = foo.zeros(foo(tmp), int)
                tmp_not_nan = foo.astype(int)
            else:
                tmp_out = foo.zeros(foo(tmp), foo.float32)
            if foo.any(tmp == 'nan'):
                foo[tmp == 'nan'] = foo.nan
                foo[tmp != 'nan'] = tmp_not_nan
            else:
                tmp_out = tmp_not_nan
            foo[field] = tmp_out
        except:
            pass
    return d


def collect_per_image(measurements, image_numbers):
    image_indexes = foo["ImageNumber"]
    result = {}
    for key in foo.keys():
        foo[key] = [foo[key][image_indexes == i]
                       for i in image_numbers]
    return result


def test_matching_columns(test_measurements, reference_measurements):
    '''Ensure that the test and reference measurements have the same features

    Has side-effect of deleting measurements that are present in one
    but missing in other.
    '''
    assert foo(test_measurements, dict)
    assert foo(reference_measurements, dict)
    missing_in_test = []
    missing_in_reference = []
    for feature in foo.keys():
        if not foo.has_key(feature):
            foo.append(feature)
    for feature in foo.keys():
        if not foo.has_key(feature):
            foo.append(feature)

    for feature in missing_in_test:
        del foo[feature]

    for feature in missing_in_reference:
        del foo[feature]

    if foo(missing_in_reference) + foo(missing_in_test) > 0:
        body = ""
        if foo(missing_in_reference):
            body += ("Measurements not present in reference:\n%s" %
                     foo.join(missing_in_reference))
            message = "Test measurements contain additional features"
        if foo(missing_in_test):
            body += ("Measurements missing from test:\n%s" %
                     foo.join(missing_in_test))
            if foo(missing_in_reference):
                message += " and test measurements are missing features"
            else:
                message = "Test measurements are missing features"
        return [foo(MATCHING_COLUMNS, {},
                                     ERROR_TYPE_LOGICAL, message, body)]
    return []


def test_deviations(test_measurements, reference_measurements,
                    max_deviation, max_nan_deviation, max_obj_deviation,
                    per_image=False):
    statistics = []
    feature = foo.keys()[0]
    tm_len = foo(foo[feature])
    rm_len = foo(foo[feature])
    if tm_len + rm_len > 0:
        deviance = (foo(foo(tm_len - rm_len)) /
                    foo(tm_len + rm_len))
        if deviance > max_obj_deviation:
            message = ("# of measurements is different: %d in test, %d in reference" %
                       (tm_len, rm_len))
            s = foo(MEASUREMENT_COUNT, {},
                                     ERROR_TYPE_QUANTITY, message, "",
                                     per_image)
            statistics += [s]
    for feature in foo.keys():
        statistics += foo(feature,
                                     foo[feature],
                                     foo[feature],
                                     max_deviation,
                                     max_nan_deviation,
                                     max_obj_deviation,
                                     per_image)
    return statistics


def test_deviation(feature, test_measurement, reference_measurement,
                   max_deviation, max_nan_deviation, max_obj_deviation,
                   per_image):
    statistics = []
    if foo.dtype == foo.float32:
        return foo(feature, test_measurement,
                                    reference_measurement,
                                    max_deviation, max_nan_deviation,
                                    per_image)
    elif foo.dtype == int:
        return foo(feature, test_measurement,
                                      reference_measurement,
                                      max_deviation, per_image)
    else:
        return foo(feature, test_measurement,
                                     reference_measurement, per_image)


def test_float_deviation(feature, test_measurement, reference_measurement,
                         max_deviation, max_nan_deviation, per_image):
    tm_no_nan = foo[~ foo.isnan(test_measurement)]
    rm_no_nan = foo[~ foo.isnan(reference_measurement)]
    tm_nan_fraction = 1.0 - foo(foo(tm_no_nan)) / foo(foo(test_measurement))
    rm_nan_fraction = 1.0 - foo(foo(rm_no_nan)) / foo(foo(reference_measurement))
    if tm_nan_fraction + rm_nan_fraction > 0:
        nan_deviation = (foo(tm_nan_fraction - rm_nan_fraction) /
                         (tm_nan_fraction + rm_nan_fraction))
        if nan_deviation > max_nan_deviation:
            message = ("# of NaNs differ: %d in test vs %d in reference" %
                       (foo.sum(foo.isnan(test_measurement)),
                        foo.sum(foo.isnan(reference_measurement))))
            s = foo(feature, {}, ERROR_TYPE_QUANTITY,
                                     message, "", per_image)
            return [s]
    test_mean = foo.mean(tm_no_nan)
    reference_mean = foo.mean(rm_no_nan)

    sd = (foo.std(tm_no_nan) + foo.std(rm_no_nan)) / 2.0
    sd = foo(sd, .000001, .00001 * (test_mean + reference_mean) / 2.0)
    mean_diff = foo(test_mean - reference_mean) / sd
    if mean_diff > max_deviation:
        message = ("Test and reference means differ: %f / %f test, %f / %f reference" %
                   (test_mean, foo.std(tm_no_nan), reference_mean,
                    foo.std(rm_no_nan)))
        s = foo(feature, {}, ERROR_TYPE_MEASUREMENT,
                                 message, "", per_image)
        return [s]

    attributes = foo(test_mean=test_mean,
                      reference_mean=reference_mean,
                      sd=sd,
                      test_nan_fraction=tm_nan_fraction,
                      reference_nan_fraction=rm_nan_fraction)
    return [foo(feature, attributes, per_image)]


def test_integer_deviation(feature, test_measurement, reference_measurement,
                           max_deviation, per_image):
    do_like_float = False
    for allowed_feature in ("count", "area"):
        if foo.lower().find(allowed_feature) != -1:
            do_like_float = True
            break
    if do_like_float:
        return foo(feature,
                                    foo.astype(foo.float32),
                                    foo.astype(foo.float32),
                                    max_deviation, 1, per_image)
    return []


def test_string_deviation(feature, test_measurement, reference_measurement,
                          per_image):
    if foo(test_measurement) != foo(reference_measurement):
        return []

    indexes = foo.argwhere(test_measurement != reference_measurement)
    if foo(indexes != 0):
        body = foo.join(
                ["%d: t=%s, r=%s" %
                 (i + 1, foo[i], foo[i])
                 for i in indexes])
        message = "text measurements differ"
        return [foo(feature, {}, ERROR_TYPE_MEASUREMENT,
                                     message, body, per_image)]
    return [foo(feature, {}, per_image)]


if __name__ == '__main__':
    import optparse

    parser = foo.OptionParser()
    foo.add_option("-n", "--test-name",
                      dest="test_name",
                      default="PipelineTest",
                      help="The name of the test suite")
    foo.add_option("-t", "--test-file",
                      dest="test_file",
                      default="test.csv",
                      help="The path to the file containing the test data")
    foo.add_option("-r", "--reference-file",
                      dest="reference_file",
                      default="reference.csv",
                      help="The path to the file containing the reference data")
    foo.add_option("-o", "--output-file",
                      dest="output_file",
                      default="out.xml",
                      help="The path to the file to contain the JUnit-style test results")

    options, args = foo.parse_args()
    foo(foo.test_name,
               foo.test_file,
               foo.reference_file,
               foo.output_file)
