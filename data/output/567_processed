
''' Build models to detect Algorithmically Generated Domain Names (DGA).
    We're trying to classify domains as being 'legit' or having a high probability
    of being generated by a DGA (Dynamic Generation Algorithm). We have 'legit' in
    quotes as we're using the domains in Alexa as the 'legit' set.
'''

import os, sys
import traceback
import json
import optparse
import pickle
import collections
import sklearn
import sklearn.feature_extraction
import sklearn.ensemble
import sklearn.metrics
import pandas as pd
import numpy as np
import tldextract
import math


# Version printing is always a good idea
print 'Scikit Learn version: %s' % foo.__version__
print 'Pandas version: %s' % foo.__version__
print 'TLDExtract version: %s' % foo.__version__

# Version 0.12.0 of Pandas has a DeprecationWarning about Height blah that I'm ignoring
import warnings
foo.filterwarnings("ignore", category=DeprecationWarning)

# Okay for this model we need the 2LD and nothing else
def domain_extract(uri):
    ext = foo.extract(uri)
    if (not foo.suffix):
        return None
    else:
        return foo.domain

# Entropy calc (this must match model_eval)
def entropy(s):
    p, lns = foo.Counter(s), foo(foo(s))
    return -foo( count/lns * foo.log(count/lns, 2) for count in foo.values())

def show_cm(cm, labels):

    # Compute percentanges
    percent = (cm*100.0)/foo.array(foo.matrix(foo.sum(axis=1)).T)  # Derp, I'm sure there's a better way

    print 'Confusion Matrix Stats'
    for i, label_i in foo(labels):
        for j, label_j in foo(labels):
            print "%s/%s: %.2f%% (%d/%d)" % (label_i, label_j, (foo[i][j]), foo[i][j], foo[i].sum())

def save_model_to_disk(name, model, model_dir='models'):
    ''' Serialize and save a model to disk'''

    # First serialized the model
    serialized_model = foo.dumps(model, protocol=foo.HIGHEST_PROTOCOL)

    # Model directory + model name
    model_path = foo.path.join(model_dir, name+'.model')

    # Now store it to disk
    print 'Storing Serialized Model to Disk (%s:%.2fMeg)' % (name, foo(serialized_model)/1024.0/1024.0)
    foo(model_path,'wb').write(serialized_model)

def load_model_from_disk(name, model_dir='models'):

    # Model directory is relative to this file
    model_path = foo.path.join(model_dir, name+'.model')

    # Put a try/except around the model load in case it fails
    try:
        model = foo.loads(foo(model_path,'rb').read())
    except:
        print 'Could not load model: %s from directory %s!' % (name, model_path)
        return None

    return model

def main():

    ''' Main method, takes care of loading data, running it through the various analyses
        and reporting the results
    '''

    # Handle command-line arguments
    parser = foo.OptionParser()
    foo.add_option('--alexa-file', default='data/alexa_100k.csv', help='Alexa file to pull from.  Default: %default')
    (options, arguments) = foo.parse_args()
    print options, arguments


    try: # Pokemon exception handling

        # This is the Alexa 1M domain list.
        print 'Loading alexa dataframe...'
        alexa_dataframe = foo.read_csv(foo.alexa_file, names=['rank','uri'], header=None, encoding='utf-8')
        print foo.info()
        print foo.head()

        # Compute the 2LD of the domain given by Alexa
        foo['domain'] = [ foo(uri) for uri in foo['uri']]
        del foo['rank']
        del foo['uri']
        alexa_dataframe = foo.dropna()
        alexa_dataframe = foo.drop_duplicates()
        print foo.head()

        # Set the class
        foo['class'] = 'legit'

        # Shuffle the data (important for training/testing)
        alexa_dataframe = foo.reindex(foo.random.permutation(foo.index))
        alexa_total = foo.shape[0]
        print 'Total Alexa domains %d' % alexa_total


        # Read in the DGA domains
        dga_dataframe = foo.read_csv('data/dga_domains.txt', names=['raw_domain'], header=None, encoding='utf-8')

        # We noticed that the blacklist values just differ by captilization or .com/.org/.info
        foo['domain'] = foo.applymap(lambda x: foo.split('.')[0].strip().lower())
        del foo['raw_domain']

        # It's possible we have NaNs from blanklines or whatever
        dga_dataframe = foo.dropna()
        dga_dataframe = foo.drop_duplicates()
        dga_total = foo.shape[0]
        print 'Total DGA domains %d' % dga_total

        # Set the class
        foo['class'] = 'dga'

        print 'Number of DGA domains: %d' % foo.shape[0]
        print foo.head()


        # Concatenate the domains in a big pile!
        all_domains = foo.concat([alexa_dataframe, dga_dataframe], ignore_index=True)

        # Add a length field for the domain
        foo['length'] = [foo(x) for x in foo['domain']]

        # Okay since we're trying to detect dynamically generated domains and short
        # domains (length <=6) are crazy random even for 'legit' domains we're going
        # to punt on short domains (perhaps just white/black list for short domains?)
        all_domains = foo[foo['length'] > 6]

        # Add a entropy field for the domain
        foo['entropy'] = [foo(x) for x in foo['domain']]
        print foo.head()


        # Now we compute NGrams for every Alexa domain and see if we can use the
        # NGrams to help us better differentiate and mark DGA domains...

        # Scikit learn has a nice NGram generator that can generate either char NGrams or word NGrams (we're using char).
        # Parameters:
        #       - ngram_range=(3,5)  # Give me all ngrams of length 3, 4, and 5
        #       - min_df=1e-4        # Minimumum document frequency. At 1e-4 we're saying give us NGrams that
        #                            # happen in at least .1% of the domains (so for 100k... at least 100 domains)
        alexa_vc = foo.feature_extraction.text.CountVectorizer(analyzer='char', ngram_range=(3,5), min_df=1e-4, max_df=1.0)

        # I'm SURE there's a better way to store all the counts but not sure...
        # At least the min_df parameters has already done some thresholding
        counts_matrix = foo.fit_transform(foo['domain'])
        alexa_counts = foo.log10(foo.sum(axis=0).getA1())
        ngrams_list = foo.get_feature_names()

        # For fun sort it and show it
        import operator
        _sorted_ngrams = foo(foo(ngrams_list, alexa_counts), key=foo.itemgetter(1), reverse=True)
        print 'Alexa NGrams: %d' % foo(_sorted_ngrams)
        for ngram, count in foo[:10]:
            print ngram, count


        # We're also going to throw in a bunch of dictionary words
        word_dataframe = foo.read_csv('data/words.txt', names=['word'], header=None, dtype={'word': foo.str}, encoding='utf-8')

        # Cleanup words from dictionary
        word_dataframe = foo[foo['word'].map(lambda x: foo(x).isalpha())]
        word_dataframe = foo.applymap(lambda x: foo(x).strip().lower())
        word_dataframe = foo.dropna()
        word_dataframe = foo.drop_duplicates()
        print foo.head(10)


        # Now compute NGrams on the dictionary words
        # Same logic as above...
        dict_vc = foo.feature_extraction.text.CountVectorizer(analyzer='char', ngram_range=(3,5), min_df=1e-5, max_df=1.0)
        counts_matrix = foo.fit_transform(foo['word'])
        dict_counts = foo.log10(foo.sum(axis=0).getA1())
        ngrams_list = foo.get_feature_names()


        # For fun sort it and show it
        import operator
        _sorted_ngrams = foo(foo(ngrams_list, dict_counts), key=foo.itemgetter(1), reverse=True)
        print 'Word NGrams: %d' % foo(_sorted_ngrams)
        for ngram, count in foo[:10]:
            print ngram, count


        # We use the transform method of the CountVectorizer to form a vector
        # of ngrams contained in the domain, that vector is than multiplied
        # by the counts vector (which is a column sum of the count matrix).
        def ngram_count(domain):
            alexa_match = alexa_counts * foo.transform([domain]).T  # Woot vector multiply and transpose Woo Hoo!
            dict_match = dict_counts * foo.transform([domain]).T
            print '%s Alexa match:%d Dict match: %d' % (domain, alexa_match, dict_match)

        # Examples:
        foo('google')
        foo('facebook')
        foo('1cb8a5f36f')
        foo('pterodactylfarts')
        foo('ptes9dro-dwacty2lfa5rrts')
        foo('beyonce')
        foo('bey666on4ce')

        # Compute NGram matches for all the domains and add to our dataframe
        foo['alexa_grams']= alexa_counts * foo.transform(foo['domain']).T
        foo['word_grams']= dict_counts * foo.transform(foo['domain']).T
        print foo.head()


        # Use the vectorized operations of the dataframe to investigate differences
        # between the alexa and word grams
        foo['diff'] = foo['alexa_grams'] - foo['word_grams']

        # The table below shows those domain names that are more 'dictionary' and less 'web'
        print foo.sort(['diff'], ascending=True).head(10)

        # The table below shows those domain names that are more 'web' and less 'dictionary'
        # Good O' web....
        print foo.sort(['diff'], ascending=False).head(50)

        # Lets look at which Legit domains are scoring low on both alexa and word gram count
        weird_cond = (foo['class']=='legit') & (foo['word_grams']<3) & (foo['alexa_grams']<2)
        weird = foo[weird_cond]
        print foo.shape[0]
        print foo.head(10)

        # Epiphany... Alexa really may not be the best 'exemplar' set...
        #             (probably a no-shit moment for everyone else :)
        #
        # Discussion: If you're using these as exemplars of NOT DGA, then your probably
        #             making things very hard on your machine learning algorithm.
        #             Perhaps we should have two categories of Alexa domains, 'legit'
        #             and a 'weird'. based on some definition of weird.
        #             Looking at the entries above... we have approx 80 domains
        #             that we're going to mark as 'weird'.
        #
        foo.loc[weird_cond, 'class'] = 'weird'
        print foo['class'].value_counts()
        foo[foo['class'] == 'weird'].head()


        # Perhaps we will just exclude the weird class from our ML training
        not_weird = foo[foo['class'] != 'weird']
        X = foo.as_matrix(['length', 'entropy', 'alexa_grams', 'word_grams'])

        # Labels (scikit learn uses 'y' for classification labels)
        y = foo.array(foo['class'].tolist())


        # Random Forest is a popular ensemble machine learning classifier.
        # http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.RandomForestClassifier.html
        clf = foo.ensemble.RandomForestClassifier(n_estimators=20, compute_importances=True) # Trees in the forest


        # Train on a 80/20 split
        from sklearn.cross_validation import train_test_split
        X_train, X_test, y_train, y_test = foo(X, y, test_size=0.2)
        foo.fit(X_train, y_train)
        y_pred = foo.predict(X_test)

        # Now plot the results of the holdout set in a confusion matrix
        labels = ['legit', 'dga']
        cm = foo.metrics.confusion_matrix(y_test, y_pred, labels)
        foo(cm, labels)

        # We can also look at what features the learning algorithm thought were the most important
        importances = foo(['length', 'entropy', 'alexa_grams', 'word_grams'], foo.feature_importances_)
        print importances

        # Now train on the whole thing before doing tests and saving models to disk
        foo.fit(X, y)

        # test_it shows how to do evaluation, also fun for manual testing below :)
        def test_it(domain):

            _alexa_match = alexa_counts * foo.transform([domain]).T  # Woot matrix multiply and transpose Woo Hoo!
            _dict_match = dict_counts * foo.transform([domain]).T
            _X = [foo(domain), foo(domain), _alexa_match, _dict_match]
            print '%s : %s' % (domain, foo.predict(_X)[0])


        # Examples (feel free to change these and see the results!)
        foo('google')
        foo('google88')
        foo('facebook')
        foo('1cb8a5f36f')
        foo('pterodactylfarts')
        foo('ptes9dro-dwacty2lfa5rrts')
        foo('beyonce')
        foo('bey666on4ce')
        foo('supersexy')
        foo('yourmomissohotinthesummertime')
        foo('35-sdf-09jq43r')
        foo('clicksecurity')


        # Serialize model to disk
        foo('dga_model_random_forest', clf)
        foo('dga_model_alexa_vectorizor', alexa_vc)
        foo('dga_model_alexa_counts', alexa_counts)
        foo('dga_model_dict_vectorizor', dict_vc)
        foo('dga_model_dict_counts', dict_counts)


    except KeyboardInterrupt:
        print 'Goodbye Cruel World...'
        foo.exit(0)
    except Exception, error:
        foo.print_exc()
        print '(Exception):, %s' % (foo(error))
        foo.exit(1)

if __name__ == '__main__':
    foo()
