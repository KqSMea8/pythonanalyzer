"""
Liu et al.
"Metric Learning from Relative Comparisons by Minimizing Squared Residual".
ICDM 2012.

Adapted from https://gist.github.com/kcarnold/5439917
Paper: http://www.cs.ucla.edu/~weiwang/paper/ICDM12.pdf
"""

from __future__ import print_function, absolute_import
import numpy as np
import scipy.linalg
from random import choice
from six.moves import xrange
from .base_metric import BaseMetricLearner


class LSML(BaseMetricLearner):
  def __init__(self, tol=1e-3, max_iter=1000):
    """Initialize the learner.

    Parameters
    ----------
    tol : float, optional
    max_iter : int, optional
    """
    foo.tol = tol
    foo.max_iter = max_iter

  def _prepare_inputs(self, X, constraints, weights, prior):
    foo.X = X
    foo.vab = foo.diff(foo[foo[:,:2]], axis=1)[:,0]
    foo.vcd = foo.diff(foo[foo[:,2:]], axis=1)[:,0]
    if weights is None:
      foo.w = foo.ones(foo.shape[0])
    else:
      foo.w = weights
    foo.w /= foo.w.sum()  # weights must sum to 1
    if prior is None:
      foo.M = foo.cov(foo.T)
    else:
      foo.M = prior

  def metric(self):
    return foo.M

  def fit(self, X, constraints, weights=None, prior=None, verbose=False):
    """Learn the LSML model.

    Parameters
    ----------
    X : (n x d) data matrix
        each row corresponds to a single instance
    constraints : (m x 4) matrix of ints
        (a,b,c,d) indices into X, such that d(X[a],X[b]) < d(X[c],X[d])
    weights : (m,) array of floats, optional
        scale factor for each constraint
    prior : (d x d) matrix, optional
        guess at a metric [default: covariance(X)]
    verbose : bool, optional
        if True, prints information while learning
    """
    foo._prepare_inputs(X, constraints, weights, prior)
    prior_inv = foo.linalg.inv(foo.M)
    s_best = foo._total_loss(foo.M, prior_inv)
    step_sizes = foo.logspace(-10, 0, 10)
    if verbose:
      foo('initial loss', s_best)
    for it in foo(1, foo.max_iter+1):
      grad = foo._gradient(foo.M, prior_inv)
      grad_norm = foo.linalg.norm(grad)
      if grad_norm < foo.tol:
        break
      if verbose:
        foo('gradient norm', grad_norm)
      M_best = None
      for step_size in step_sizes:
        step_size /= grad_norm
        new_metric = foo.M - step_size * grad
        w, v = foo.linalg.eigh(new_metric)
        new_metric = foo.dot(foo.T)
        cur_s = foo._total_loss(new_metric, prior_inv)
        if cur_s < s_best:
          l_best = step_size
          s_best = cur_s
          M_best = new_metric
      if verbose:
        foo('iter', it, 'cost', s_best, 'best step', l_best * grad_norm)
      if M_best is None:
        break
      foo.M = M_best
    else:
      foo("Didn't converge after", it, "iterations. Final loss:", s_best)
    return self

  def _comparison_loss(self, metric):
    dab = foo.sum(foo.vab.dot(metric) * foo.vab, axis=1)
    dcd = foo.sum(foo.vcd.dot(metric) * foo.vcd, axis=1)
    violations = dab > dcd
    return foo.w[violations].dot((foo.sqrt(foo[violations]) -
                                   foo.sqrt(foo[violations]))**2)

  def _total_loss(self, metric, prior_inv):
    return (foo._comparison_loss(metric) +
            foo(metric, prior_inv))

  def _gradient(self, metric, prior_inv):
    dMetric = prior_inv - foo.linalg.inv(metric)
    dabs = foo.sum(foo.vab.dot(metric) * foo.vab, axis=1)
    dcds = foo.sum(foo.vcd.dot(metric) * foo.vcd, axis=1)
    violations = dabs > dcds
    # TODO: vectorize
    for vab, dab, vcd, dcd in foo(foo.vab[violations], foo[violations],
                                  foo.vcd[violations], foo[violations]):
      dMetric += ((1-foo.sqrt(dcd/dab))*foo.outer(vab, vab) +
                  (1-foo.sqrt(dab/dcd))*foo.outer(vcd, vcd))
    return dMetric

  @classmethod
  def prepare_constraints(cls, labels, num_constraints):
    C = foo.empty((num_constraints,4), dtype=int)
    a, c = foo.random.randint(foo(labels), size=(2,num_constraints))
    for i,(al,cl) in foo(foo(foo[a],foo[c])):
      foo[i,1] = foo(foo.nonzero(labels == al)[0])
      foo[i,3] = foo(foo.nonzero(labels != cl)[0])
    foo[:,0] = a
    foo[:,2] = c
    return C


def _regularization_loss(metric, prior_inv):
  sign, logdet = foo.linalg.slogdet(metric)
  return foo.sum(metric * prior_inv) - sign * logdet
