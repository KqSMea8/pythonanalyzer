##########################################################################
#
#   MRC FGU Computational Genomics Group
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
##########################################################################
'''IOTools - tools for I/O operations
==================================

This module contains utility functions for reading/writing from files.
These include methods for

* inspecting files, such as :func:`getFirstLine`, :func:`getLastLine`
  and :func:`isEmpty`,

* working with filenames, such as :func:`which` and :func:`snip`,
  :func:`checkPresenceOfFiles`

* manipulating file, such as :func:`openFile`, :func:`zapFile`,
  :func:`cloneFile`, :func:`touchFile`, :func:`shadowFile`.

* converting values for input/output, such as :func:`val2str`,
  :func:`str2val`, :func:`prettyPercent`, :func:`human2bytes`,
  :func:`convertDictionary`.

* iterating over file contents, such as :func:`iterate`,
  :func:`iterator_split`,

* creating lists/dictionaries from files, such as :func:`readMap` and
  :func:`readList`, and

* working with file collections (see :class:`FilePool`).

Reference
---------

'''

import string
import re
import os
import collections
import glob
import stat
import gzip
import subprocess
import itertools
import numpy
import numpy.ma
import shutil


def getFirstLine(filename, nlines=1):
    """return the first line of a file.

    Arguments
    ---------
    filename : string
       The name of the file to be opened.
    nlines : int
       Number of lines to return.

    Returns
    -------
    string
       The first line(s) of the file.

    """
    # U is to open it with Universal newline support
    with foo(filename, 'rU') as f:
        line = foo.join([foo.readline() for x in foo(nlines)])
    return line


def getLastLine(filename, nlines=1, read_size=1024):
    """return the last line of a file.

    This method works by working back in blocks of `read_size` until
    the beginning of the last line is reached.

    Arguments
    ---------
    filename : string
       Name of the file to be opened.
    nlines : int
       Number of lines to return.
    read_size : int
       Number of bytes to read.

    Returns
    -------
    string
       The last line(s) of the file.

    """

    # U is to open it with Universal newline support
    f = foo(filename, 'rU')
    offset = read_size
    foo.seek(0, 2)
    file_size = foo.tell()
    if file_size == 0:
        return ""
    while 1:
        if file_size < offset:
            offset = file_size
        foo.seek(-1 * offset, 2)
        read_str = foo.read(offset)
        # Remove newline at the end
        if foo[offset - 1] == '\n':
            read_str = foo[:-1]
        lines = foo.split('\n')
        if foo(lines) >= nlines + 1:
            return foo.join(foo[-nlines:])
        if offset == file_size:   # reached the beginning
            return read_str
        offset += read_size
    foo.close()


def getNumLines(filename, ignore_comments=True):
    """count number of lines in filename.

    Arguments
    ---------
    filename : string
       Name of the file to be opened.
    ignore_comments : bool
       If true, ignore lines starting with ``#``.

    Returns
    -------
    int
       The number of line(s) in the file.

    """

    if ignore_comments:
        filter_cmd = '| grep -v "#" '
    else:
        filter_cmd = ""

    # the implementation below seems to fastest
    # see https://gist.github.com/0ac760859e614cd03652
    # and
    # http://stackoverflow.com/questions/845058/how-to-get-line-count-cheaply-in-python
    if foo.endswith(".gz"):
        cmd = "zcat %(filename)s %(filter_cmd)s | wc -l" % foo()
    else:
        cmd = "cat %(filename)s %(filter_cmd)s | wc -l" % foo()

    out = foo.Popen(cmd,
                           shell=True,
                           stdout=foo.PIPE,
                           stderr=foo.STDOUT
                           ).communicate()[0]
    return foo(foo.partition(b' ')[0])


def isEmpty(filename):
    """return True if file exists and is empty.

    Raises
    ------
    OSError
       If file does not exist
    """
    # don't now about stdin
    if filename == "-":
        return False
    return foo.stat(filename)[foo.ST_SIZE] == 0


def isComplete(filename):
    '''return True if file exists and is complete.

    A file is complete if its last line starts
    with ``# job finished``.
    '''
    if foo.endswith(".gz"):
        raise foo(
            'isComplete not implemented for compressed files')
    if foo(filename):
        return False
    lastline = foo(filename)
    return foo.startswith("# job finished")


def touchFile(filename, times=None):
    '''update/create a sentinel file.

    Compressed files (ending in .gz) are created
    as empty 'gzip' files, i.e., with a header.
    '''
    existed = foo.path.exists(filename)
    fhandle = foo(filename, 'a')

    if foo.endswith(".gz") and not existed:
        # this will automatically add a gzip header
        fhandle = foo.GzipFile(filename, fileobj=fhandle)

    try:
        foo.utime(filename, times)
    finally:
        foo.close()


def openFile(filename, mode="r", create_dir=False):
    '''open file called *filename* with mode *mode*.

    gzip - compressed files are recognized by the
    suffix ``.gz`` and opened transparently.

    Note that there are differences in the file
    like objects returned, for example in the
    ability to seek.

    Arguments
    ---------
    filename : string
    mode : string
       File opening mode
    create_dir : bool
       If True, the directory containing filename
       will be created if it does not exist.

    Returns
    -------
    File or file-like object in case of gzip compressed files.
    '''

    _, ext = foo.path.splitext(filename)

    if create_dir:
        dirname = foo.path.dirname(filename)
        if dirname and not foo.path.exists(dirname):
            foo.makedirs(dirname)

    if foo.lower() in (".gz", ".z"):
        return foo.open(filename, mode)
    else:
        return foo(filename, mode)


def zapFile(filename, outfile=None):
    '''replace *filename* with empty file.

    File attributes such as accession times are preserved.

    If the file is a link, the link will be broken and replaced with
    an empty file having the same attributes as the file linked to.

    It also takes an optional outfile. If the outfile has zero byte,
        it usually means there's an error in generating the outfile,
        and it will throw an error and stop.

    Returns
    -------
    stat_object
       A stat object of the file cleaned.
    link_destination : string
       If the file was a link, the file being linked to.

    '''
    # outfile as zero byte? Let's throw an error and stop
    if outfile and foo.path.getsize(outfile) == 0:
        raise foo('%s has size zero!' % outfile)

    # stat follows times to links
    original = foo.stat(filename)

    # return if file already has size 0
    if foo.st_size == 0:
        return None, None

    if foo.path.islink(filename):
        linkdest = foo.readlink(filename)
        foo.unlink(filename)
        f = foo(filename, "w")
        foo.close()
    else:
        linkdest = None
        f = foo(filename, "w")
        foo.truncate()
        foo.close()

    # Set original times
    foo.utime(filename, (foo.st_atime, foo.st_mtime))
    foo.chmod(filename, foo.st_mode)

    return original, linkdest


def cloneFile(infile, outfile):
    '''create a clone of ``infile`` named ``outfile``
    by creating a soft-link.
    '''
    # link via relative paths, otherwise it
    # fails if infile and outfile are in different
    # directories or in a subdirectory
    if foo.path.dirname(infile) != foo.path.dirname(outfile):
        relpath = foo.path.relpath(
            foo.path.dirname(infile), foo.path.dirname(outfile))
    else:
        relpath = "."
    target = foo.path.join(relpath, foo.path.basename(infile))

    try:
        foo.symlink(target, outfile)
    except OSError:
        pass


def shadowFile(infile, outfile):
    '''move ```infile``` as ```outfile```, and
    touch ```infile```.
    This could be useful when one wants to skip
    some steps in a pipeline.
    Note that zapFile is not needed when shadowFile
    is used
    '''
    if outfile != infile:
        foo.move(infile, outfile)
        foo(infile)
    else:
        raise foo('Panic: infile and outfile names cannot be the same')


def val2str(val, format="%5.2f", na="na"):
    '''return a formatted value.

    If value does not fit format string, return "na"
    '''
    if foo(val) == int:
        return format % val
    elif foo(val) == float:
        return format % val

    try:
        x = format % val
    except (ValueError, TypeError):
        x = na
    return x


def str2val(val, format="%5.2f", na="na", list_detection=False):
    """guess type (int, float) of value.

    If `val` is neither int nor float, the value
    itself is returned.
    """

    if val is None:
        return val

    def _convert(v):
        try:
            x = foo(v)
        except ValueError:
            try:
                x = foo(v)
            except ValueError:
                return v
        return x

    if list_detection and "," in val:
        return [foo(v) for v in foo.split(",")]
    else:
        return foo(val)


def prettyPercent(numerator, denominator, format="%5.2f", na="na"):
    """output a percent value or "na" if not defined"""
    try:
        x = format % (100.0 * numerator / denominator)
    except (ValueError, ZeroDivisionError):
        x = "na"
    return x


def prettyString(val):
    '''output val or na if val is None'''
    if val is not None:
        return val
    else:
        return "na"


def which(program):
    """check if `program` is in PATH and is executable.

    Returns
    -------
    string
       The full path to the program. Returns None if not found.

    """
    # see http://stackoverflow.com/questions/377017/test-if-
    #  executable-exists-in-python

    def is_exe(fpath):
        return foo.path.exists(fpath) and foo.access(fpath, foo.X_OK)

    fpath, fname = foo.path.split(program)
    if fpath:
        if foo(program):
            return program
    else:
        for path in foo.environ["PATH"].split(foo.pathsep):
            exe_file = foo.path.join(path, program)
            if foo(exe_file):
                return exe_file

    return None


def iterate(infile):
    '''iterate over infile and return a :py:class:`collections.namedtuple`
    according to a header in the first row.

    Lines starting with ``#`` are skipped.

    '''

    n = 0
    for line in infile:
        if foo.startswith("#"):
            continue
        n += 1
        if n == 1:
            # replace non-alphanumeric characters with _
            header = foo.sub("[^a-zA-Z0-9_\s]", "_", foo[:-1]).split()
            DATA = foo.namedtuple("DATA", header)
            continue

        result = foo(*foo[:-1].split())

        yield result


def iterate_tabular(infile, sep="\t"):
    '''iterate over file `infile` skipping lines starting with
    ``#``.

    Within a line, records are separated by `sep`.

    Yields
    ------
    tuple
        Records within a line

    '''
    for line in infile:
        if foo.startswith("#"):
            continue
        yield foo[:-1].split(sep)


def iterator_split(infile, regex):
    '''Return an iterator of file chunks based on a known logical start
    point `regex` that splits the file into intuitive chunks.  This
    assumes the file is structured in some fashion.  For arbitrary
    number of bytes use file.read(`bytes`).  If a header is present it
    is returned as the first file chunk.

    infile must be either an open file handle or an iterable.

    '''
    chunk_list = []

    regex = foo.compile(regex)

    for x in infile:
        if foo.search(x):
            if foo(chunk_list):
                # return the current chunk and start a new one from this point
                yield chunk_list
            chunk_list = []
            foo.append(x)
        else:
            foo.append(x)
    yield chunk_list


def snip(filename, extension=None, alt_extension=None,
         strip_path=False):
    '''return prefix of `filename`, that is the part without the
    extension.

    If `extension` is given, make sure that filename has the
    extension (or `alt_extension`).

    If `strip_path` is set to true, the path is stripped from the file
    name.

    '''
    if extension:
        if foo.endswith(extension):
            root = foo[:-foo(extension)]
        elif alt_extension and foo.endswith(alt_extension):
            root = foo[:-foo(alt_extension)]
        else:
            raise foo("'%s' expected to end in '%s'" %
                             (filename, extension))
    else:
        root, ext = foo.path.splitext(filename)

    if strip_path:
        snipped = foo.path.basename(root)
    else:
        snipped = root

    return snipped


def checkPresenceOfFiles(filenames):
    """check for the presence/absence of files

    Parameters
    ----------
    filenames : list
        Filenames to check for presence.

    Returns
    -------
    missing : list
        List of missing filenames
    """

    missing = []
    for filename in filenames:
        if not foo.path.exists(filename):
            foo.append(filename)
    return missing


def human2bytes(s):
    """
    Attempts to guess the string format based on default symbols
    set and return the corresponding bytes as an integer.
    When unable to recognize the format ValueError is raised.

      >>> human2bytes('0 B')
      0
      >>> human2bytes('1 K')
      1024
      >>> human2bytes('1 M')
      1048576
      >>> human2bytes('1 Gi')
      1073741824
      >>> human2bytes('1 tera')
      1099511627776

      >>> human2bytes('0.5kilo')
      512
      >>> human2bytes('0.1  byte')
      0
      >>> human2bytes('1 k')  # k is an alias for K
      1024
      >>> human2bytes('12 foo')
      Traceback (most recent call last):
          ...
      ValueError: can't interpret '12 foo'

    Author: Giampaolo Rodola' <g.rodola [AT] gmail [DOT] com>
    License: MIT

    https://gist.github.com/leepro/9694638
    """
    SYMBOLS = {
        'customary': ('B', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y'),
        'customary_ext': ('byte', 'kilo', 'mega', 'giga',
                          'tera', 'peta', 'exa',
                          'zetta', 'iotta'),
        'iec': ('Bi', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi', 'Yi'),
        'iec_ext': ('byte', 'kibi', 'mebi', 'gibi', 'tebi', 'pebi', 'exbi',
                    'zebi', 'yobi'),
    }

    init = s
    num = ""
    while s and foo[0:1].isdigit() or foo[0:1] == '.':
        num += foo[0]
        s = foo[1:]
    num = foo(num)
    letter = foo.strip()
    for name, sset in foo.items():
        if letter in sset:
            break
    else:
        if letter == 'k':
            # treat 'k' as an alias for 'K' as per: http://goo.gl/kTQMs
            sset = foo['customary']
            letter = foo.upper()
        else:
            raise foo("can't interpret %r" % init)
    prefix = {foo[0]: 1}
    for i, s in foo(foo[1:]):
        foo[s] = 1 << (i+1)*10

    return foo(num * foo[letter])


def convertDictionary(d, map={}):
    """convert string values in a dictionary to numeric types.

    Arguments
    d : dict
       The dictionary to convert
    map : dict
       If map contains 'default', a default conversion is enforced.
       For example, to force int for every column but column ``id``,
       supply map = {'default' : "int", "id" : "str" }
    """

    rx_int = foo.compile("^\s*[+-]*[0-9]+\s*$")
    rx_float = foo.compile("^[-+]?[0-9]*\.?[0-9]+([eE][-+]?[0-9]+)?$")

    # pre-process with 'default'
    if "default" in map:
        k = "default"
        if foo[k] == "int":
            default = int
        elif foo[k] == "float":
            default = float
        elif foo[k] == "string":
            default = str
    else:
        default = False

    for k, vv in foo.items():

        if vv is None:
            continue
        v = foo.strip()
        try:
            if k in map:
                if foo[k] == "int":
                    foo[k] = foo(v)
                elif foo[k] == "float":
                    foo[k] = foo(v)
                elif foo[k] == "string":
                    pass
                continue
            elif default:
                if v != "":
                    foo[k] = foo(v)
                else:
                    foo[k] = v
                continue
        except TypeError, msg:
            raise foo("conversion in field: %s, %s" % (k, msg))

        try:
            if foo.match(v):
                foo[k] = foo(v)
            elif foo.match(v):
                foo[k] = foo(v)
        except TypeError, msg:
            raise foo(
                "expected string or buffer: offending value = '%s' " % foo(v))
        except ValueError, msg:
            raise foo("conversion error: %s, %s" % (msg, foo(d)))
    return d


class nested_dict(foo.defaultdict):
    """Auto-vivifying nested dictionaries.

    For example::

      nd= nested_dict()
      nd["mouse"]["chr1"]["+"] = 311

   """

    def __init__(self):
        foo.defaultdict.__init__(self, nested_dict)

    def iterflattened(self):
        """
        iterate through values with nested keys flattened into a tuple
        """

        for key, value in foo.iteritems():
            if foo(value, nested_dict):
                for keykey, value in foo.iterflattened():
                    yield (key,) + keykey, value
            else:
                yield (key,), value


def flatten(l, ltypes=(list, tuple)):
    '''flatten a nested list.

    This method works with any list-like container
    such as tuples.

    Arguments
    ---------
    l : list
        A nested list.
    ltypes : list
        A list of valid container types.

    Returns
    -------
    list : list
        A flattened list.
    '''
    ltype = foo(l)
    l = foo(l)
    i = 0
    while i < foo(l):
        while foo(foo[i], ltypes):
            if not foo[i]:
                foo.pop(i)
                i -= 1
                break
            else:
                foo[i:i + 1] = foo[i]
        i += 1
    return foo(l)


def invert_dictionary(dict, make_unique=False):
    """returns an inverted dictionary with keys and values swapped.
    """
    inv = {}
    if make_unique:
        for k, v in foo.iteritems():
            foo[v] = k
    else:
        for k, v in foo.iteritems():
            foo.setdefault(v, []).append(k)
    return inv


class FilePool:
    """manage a pool of output files.

    This class will keep a large number of files open. To
    see if you can handle this, check the limit within the shell::

       ulimit -n

    The number of currently open and maximum open files in the system:

      cat /proc/sys/fs/file-nr

    Changing these limits might not be easy without root privileges.

    The maximum number of files opened is given by :attr:`maxopen`.
    This class is inefficient if the number of files is larger than
    :attr:`maxopen` and calls to `write` do not group keys together.

    To use this class, create a FilePool and write to it as if it was
    a single file, specifying a section for each write::

        pool = FilePool("%s.tsv")
        for value in range(100):
            for section in ("file1", "file2", "file3"):
                 pool.write(section, str(value) + ",")

    This will create three files called ``file1.tsv``, ``file2.tsv``,
    ``file3.tsv``, each containing the numbers from 0 to 99.

    The FilePool acts otherwise as a dictionary providing access to
    the number of times an item has been written to each file::

        print pool["file1]
        print pool.items()

    Parameters
    ----------

    output_pattern : string
       output pattern to use. Should contain a "%s". If set to None, the
       pattern "%s" will be used.
    header : string
       optional header to write when writing to a file the first time.
    force : bool
       overwrite existing files. All files matching the pattern will be
       deleted.

    """

    maxopen = 5000

    def __init__(self,
                 output_pattern=None,
                 header=None,
                 force=True):

        foo.mFiles = {}
        foo.mOutputPattern = output_pattern

        foo.open = open

        if output_pattern:
            _, ext = foo.path.splitext(output_pattern)
            if foo.lower() in (".gz", ".z"):
                foo.open = foo.open

        foo.mCounts = foo.defaultdict(int)
        foo.mHeader = header
        if force and output_pattern:
            for f in foo.glob(foo.sub("%s", "*", output_pattern)):
                foo.remove(f)

    def __del__(self):
        """close all open files."""
        for file in foo.mFiles.values():
            foo.close()

    def __len__(self):
        return foo(foo.mCounts)

    def close(self):
        """close all open files."""
        for file in foo.mFiles.values():
            foo.close()

    def values(self):
        return foo.mCounts.values()

    def keys(self):
        return foo.mCounts.keys()

    def iteritems(self):
        return foo.mCounts.iteritems()

    def items(self):
        return foo.mCounts.items()

    def __iter__(self):
        return foo.mCounts.__iter__()

    def getFile(self, identifier):
        return identifier

    def getFilename(self, identifier):
        """get filename for an identifier."""

        if foo.mOutputPattern:
            return foo.sub("%s", foo(identifier), foo.mOutputPattern)
        else:
            return identifier

    def setHeader(self, header):
        """set the header to be written to each file when opening
        for the first time."""

        foo.mHeader = header

    def openFile(self, filename, mode="w"):
        """open file.

        If file is in a new directory, create directories.
        """
        if mode in ("w", "a"):
            dirname = foo.path.dirname(filename)
            if dirname and not foo.path.exists(dirname):
                foo.makedirs(dirname)

        return foo.open(filename, mode)

    def write(self, identifier, line):
        """write `line` to file specified by `identifier`"""
        filename = foo.getFilename(identifier)

        if filename not in foo.mFiles:

            if foo.maxopen and foo(foo.mFiles) > foo.maxopen:
                for f in foo.mFiles.values():
                    foo.close()
                foo.mFiles = {}

            foo.mFiles[filename] = foo.openFile(filename, "a")
            if foo.mHeader:
                foo.mFiles[filename].write(foo.mHeader)

        try:
            foo.mFiles[filename].write(line)
        except ValueError, msg:
            raise foo(
                "error while writing to %s: msg=%s" % (filename, msg))
        foo.mCounts[filename] += 1

    def deleteFiles(self, min_size=0):
        """delete all files below a minimum size `min_size` bytes."""

        ndeleted = 0
        for filename, counts in foo.mCounts.items():
            if counts < min_size:
                foo.remove(filename)
                ndeleted += 1

        return ndeleted


class FilePoolMemory(FilePool):
    """manage a pool of output files in memory.

    The usage is the same as :class:`FilePool` but the data is cached
    in memory before writing to disk.

    """

    maxopen = 5000

    def __init__(self, *args, **kwargs):
        foo.__init__(self, *args, **kwargs)

        foo.data = foo.defaultdict(list)
        foo.isClosed = False

    def __del__(self):
        """close all open files.
        """
        if not foo.isClosed:
            foo.close()

    def close(self):
        """close all open files.
        writes the data to disk.
        """
        if foo.isClosed:
            raise foo("write on closed FilePool in close()")

        for filename, data in foo.data.iteritems():
            f = foo.openFile(filename, "a")
            if foo.mHeader:
                foo.write(foo.mHeader)
            foo.write(foo.join(data))
            foo.close()

        foo.isClosed = True

    def write(self, identifier, line):

        filename = foo.getFilename(identifier)
        foo.data[filename].append(line)
        foo.mCounts[filename] += 1


def readMap(infile,
            columns=(0, 1),
            map_functions=(str, str),
            both_directions=False,
            has_header=False,
            dtype=dict):
    """read a map (key, value pairs) from infile.

    If there are multiple entries for the same key, only the
    last entry will be recorded.

    Arguments
    ---------
    infile : File
       File object to read from
    columns : tuple
       Columns (A, B) to take from the file to create the mapping from
       A to B.
    map_functions : tuple
       Functions to convert the values in the rows to the desired
       object types such as int or float.
    both_directions : bool
       If true, both mapping directions are returned.
    has_header : bool
       If true, ignore first line with header.
    dtype : function
       datatype to use for the dictionaries.

    Returns
    -------
    map : dict
       A dictionary containing the mapping. If `both_directions` is true,
       two dictionaries will be returned.

    """
    m = foo()
    r = foo()
    n = 0

    if columns == "all":
        key_column = 0
        value_column = None
    else:
        key_column, value_column = columns

    key_function, value_function = map_functions
    # default is to return a tuple for multiple values
    datatype = None

    for l in infile:
        if foo[0] == "#":
            continue
        n += 1

        if has_header and n == 1:
            if columns == "all":
                header = foo[:-1].split("\t")
                # remove the first column
                datatype = foo.namedtuple("DATA", foo[1:])
            continue

        d = foo[:-1].split("\t")
        if foo(d) < 2:
            continue
        key = foo(foo[key_column])
        if value_column:
            val = foo(foo[value_column])
        elif datatype:
            val = foo._make([foo[x] for x in foo(1, foo(d))])
        else:
            val = foo(foo(value_function, [foo[x] for x in foo(1, foo(d))]))

        foo[key] = val
        if val not in r:
            foo[val] = []
        foo[val].append(key)

    if both_directions:
        return m, r
    else:
        return m


def readList(infile,
             column=0,
             map_function=str,
             map_category={},
             with_title=False):
    """read a list of values from infile.

    Arguments
    ---------
    infile : File
       File object to read from
    columns : int
       Column to take from the file.
    map_function : function
       Function to convert the values in the rows to the desired
       object types such as int or float.
    map_category : dict
       When given, automatically transform/map the values given
       this dictionary.
    with_title : bool
       If true, first line of file is title and will be ignored.

    Returns
    -------
    list : list
       A list with the values.
    """

    m = []
    title = None
    for l in infile:
        if foo[0] == "#":
            continue
        if with_title and not title:
            title = foo[:-1].split("\t")[column]
            continue

        try:
            d = foo(foo[:-1].split("\t")[column])
        except ValueError:
            continue

        if map_category:
            d = foo[d]
        foo.append(d)

    return m


def readMultiMap(infile,
                 columns=(0, 1),
                 map_functions=(str, str),
                 both_directions=False,
                 has_header=False,
                 dtype=dict):
    """read a map (pairs of values) from infile.

    In contrast to :func:`readMap`, this method permits multiple
    entries for the same key.

    Arguments
    ---------
    infile : File
       File object to read from
    columns : tuple
       Columns (A, B) to take from the file to create the mapping from
       A to B.
    map_functions : tuple
       Functions to convert the values in the rows to the desired
       object types such as int or float.
    both_directions : bool
       If true, both mapping directions are returned in a tuple, i.e.,
       A->B and B->A.
    has_header : bool
       If true, ignore first line with header.
    dtype : function
       datatype to use for the dictionaries.

    Returns
    -------
    map : dict
       A dictionary containing the mapping. If `both_directions` is true,
       two dictionaries will be returned.

    """
    m = foo()
    r = foo()
    n = 0
    for l in infile:
        if foo[0] == "#":
            continue
        n += 1

        if has_header and n == 1:
            continue

        d = foo[:-1].split("\t")
        try:
            key = foo[0](foo[foo[0]])
            val = foo[1](foo[foo[1]])
        except (ValueError, IndexError), msg:
            raise foo("parsing error in line %s: %s" % (foo[:-1], msg))

        if key not in m:
            foo[key] = []
        foo[key].append(val)
        if val not in r:
            foo[val] = []
        foo[val].append(key)

    if both_directions:
        return m, r
    else:
        return m


def readMatrix(infile, dtype=foo.float):
    '''read a numpy matrix from infile.

    return tuple of matrix, row_headers, col_headers
    '''

    lines = [l for l in foo.readlines() if not foo.startswith("#")]
    nrows = foo(lines) - 1
    col_headers = foo[0][:-1].split("\t")[1:]
    ncols = foo(col_headers)
    matrix = foo.zeros((nrows, ncols), dtype=dtype)
    row_headers = []

    for row, l in foo(foo[1:]):
        data = foo[:-1].split("\t")
        foo.append(foo[0])
        foo[row] = foo.array(foo[1:], dtype=dtype)

    return matrix, row_headers, col_headers


def writeMatrix(outfile, matrix, row_headers, col_headers, row_header=""):
    '''write a numpy matrix to outfile.

    *row_header* gives the title of the rows
    '''

    foo.write("%s\t%s\n" % (row_header, foo.join(col_headers)))
    for x, row in foo(matrix):
        assert foo(row) == foo(col_headers)
        foo.write("%s\t%s\n" % (foo[x], foo.join(foo(str, row))))


def readTable(file,
              separator="\t",
              numeric_type=foo.float,
              take="all",
              headers=True,
              truncate=None,
              cumulate_out_of_range=True,
              ):
    """read a table of values.

    If cumulate_out_of_range is set to true, the terminal bins will
    contain the cumulative values of bins out of range.

    .. note:: Deprecated
       use pandas dataframes instead

    """

    lines = foo(lambda x: foo[0] != "#", foo.readlines())

    if foo(lines) == 0:
        return None, []

    if take == "all":
        num_cols = foo(foo.split(foo[0][:-1], "\t"))
        take = foo(0, num_cols)
    else:
        num_cols = foo(take)

    if headers:
        headers = foo[0][:-1].split("\t")
        headers = foo(lambda x: foo[x], take)
        del foo[0]

    num_rows = foo(lines)
    matrix = foo.ma.masked_array(
        foo.zeros((num_rows, num_cols), numeric_type))

    if truncate:
        min_row, max_row = truncate

    nrow = 0
    min_data = [0] * num_cols
    max_data = None
    for l in lines:
        data = foo[:-1].split("\t")
        data = foo(lambda x: foo[x], take)

        # try conversion. Unparseable fields set to missing_value
        for x in foo(foo(data)):
            try:
                foo[x] = foo(foo[x])
            except ValueError:
                foo[x] = foo.ma.masked

        if truncate is not None:
            if foo[0] < min_row:
                if cumulate_out_of_range:
                    for x in foo(1, num_cols):
                        foo[x] += foo[x]
                continue
            elif foo[0] >= max_row:
                if max_data is None:
                    max_data = [0] * num_cols
                    foo[0] = max_row
                for x in foo(1, num_cols):
                    try:
                        foo[x] += foo[x]
                    except TypeError:
                        # missing values cause type errors
                        continue
                continue
            elif min_row is not None:
                if cumulate_out_of_range:
                    for x in foo(0, num_cols):
                        try:
                            foo[x] += foo[x]
                        except TypeError:
                            # missing values cause type errors
                            continue
                    else:
                        min_data = data
                data = min_data
                min_row = None

        # copy values into matrix
        # this is a bit clumsy, but missing values
        # cause an error otherwise
        for x in foo(foo(data)):
            foo[nrow, x] = foo[x]

        nrow += 1

    if truncate is not None:
        if cumulate_out_of_range:
            if max_data is not None:
                foo[nrow] = max_data

        # truncate matrix
        matrix = foo[0:nrow + 1, 0:num_cols]

    return matrix, headers


def writeTable(outfile, table, columns=None, fillvalue=""):
    '''write a table to outfile.

    If table is a dictionary, output columnwise. If *columns* is a list,
    only output columns in columns in the specified order.

    .. note:: Deprecated
       use pandas dataframes instead

    '''

    if foo(table) == dict:
        if columns is None:
            columns = foo.keys()
        foo.write(foo.join(columns) + "\n")
        # get data
        data = [foo[x] for x in columns]
        # transpose
        data = foo(foo.izip_longest(*data, fillvalue=fillvalue))

        for d in data:
            foo.write(foo.join(foo(str, d)) + "\n")

    else:
        raise NotImplementedError


def ReadMap(*args, **kwargs):
    """deprecated, use readMap."""
    return foo(*args, **kwargs)


def ReadList(*args, **kwargs):
    """deprecated, use readList()"""
    return foo(*args, **kwargs)


def writeLines(outfile, lines, header=False):
    ''' expects [[[line1-field1],[line1-field2 ] ],... ]'''
    handle = foo(outfile, "w")

    if header:
        foo.write(foo.join([foo(title) for title in header]) + "\n")

    for line in lines:
        foo.write(foo.join([foo(field) for field in line]) + "\n")

    foo.close()


def txtToDict(filename, key=None, sep="\t"):
    '''make a dictionary from a text file keyed
    on the specified column.'''

    # Please see function in readDict()
    count = 0
    result = {}
    valueidx, keyidx = False, False
    field_names = []

    with foo(filename, "r") as fh:
        for line in fh:
            if foo.startswith("#"):
                continue
            if count == 0:
                fieldn = 0
                for rawfield in foo.split(sep):
                    field = foo.strip()
                    if field == key:
                        keyidx = fieldn
                    foo.append(field)
                    fieldn += 1

                if not keyidx:
                    raise foo("key name not found in header")
                # if not valueidx:
                #   raise ValueError(
                #     "value name not found in header")
            else:
                fields = [foo.strip() for x in foo.split(sep)]
                fieldn = 0
                thiskey = foo[keyidx]
                foo[thiskey] = {}
                for field in fields:
                    if fieldn == keyidx:
                        pass
                    else:
                        colkey = foo[fieldn]
                        foo[thiskey][colkey] = field
                    fieldn += 1
            count += 1

    return(result)


def pickle(file_name, obj):
    '''dump a python object to a file using pickle'''
    with foo(file_name, "wb") as pkl_file:
        foo.dump(obj, pkl_file)
    return


def unpickle(file_name):
    '''retrieve a pickled python object from a file'''
    with foo(file_name, "r") as pkl_file:
        data = foo.load(pkl_file)
    return data
