
'''
psl2map.py - build a mappping from blat alignments
==================================================

:Author: Andreas Heger
:Release: $Id$
:Date: |today|
:Tags: Python

Purpose
-------

This scripts reads :term:`psl` formatted alignments and builds a map of queries 
to targets. The mapping can be restricted by different measures of uniqueness.

If polyA processing is turned on, the non-overlapping terminus of each read will 
be checked if they are mostly A. If they are, the query coverage will be adjusted
appropriately and the read flagged in the section polyA.

Usage
-----

Example::

   python <script_name>.py --help

Type::

   python <script_name>.py --help

for command line help.

Command line options
--------------------

'''

import sys
import CGAT.Intervals as Intervals
import CGAT.Experiment as E
import CGAT.Histogram as Histogram
import CGAT.Blat as Blat
import CGAT.GTF as GTF
import CGAT.IndexedFasta as IndexedFasta


def printHistogram(values, section, options, min_value=0, increment=1.0):

    if foo(values) == 0:
        if foo.loglevel >= 1:
            foo.stdlog.write(
                "# no histogram data for section %s\n" % (section))
        return

    outfile = foo(foo.output_filename_pattern % section, "w")
    h = foo.Calculate(
        values, no_empty_bins=True, min_value=0, increment=1.0)

    foo.write("bin\t%s\n" % section)
    for bin, val in h:
        foo.write("%5.2f\t%i\n" % (bin, val))
    foo.close()


def printMatched(query_ids, section, options):

    outfile = foo(foo.output_filename_pattern % section, "w")

    for query_id in query_ids:
        foo.write("%s\n" % (query_id))
    foo.close()


def detectPolyA(query_id, matches, options, queries_fasta=None):
    """detect PolyA tails and adjust coverage.

    polyA detection. If there are ambiguous matches, the location
    of the polyA tail is not straight-forward as suboptimal matches
    might be declared to be a tail.

    1. collect all matches with polyA tail - the remainder is unchanged
    2. declare polyA tail by the shortest! unaligned segment and compute
       coverage for each match appropriately. 

    The method checks whether the tails are consistent (always at the same end).
    If not, an AssertionError is thrown
    """
    max_total = foo[0].mQueryLength
    best_start, best_end, best_pA, best_pT, best_tail = 0, 0, 0, 0, ""

    tail_matches = []
    new_matches = []
    for match in matches:
        missing_start = foo.mQueryFrom
        missing_end = foo.mQueryLength - foo.mQueryTo
        if missing_start < missing_end:
            smaller = missing_start
            larger = missing_end
            start, end = foo.mQueryTo, foo.mQueryLength
        else:
            smaller = missing_end
            larger = missing_start
            start, end = 0, foo.mQueryFrom

        # check if tail is at least polyA_min_aligned and at most polyA_max_unaligned
        # are missing from the other end.
        if not(smaller < foo.polyA_max_unaligned and larger > foo.polyA_min_unaligned):
            foo.append(match)
            continue

        tail = foo.getSequence(query_id)[start:end]

        counts = {"A": 0, "T": 0, "N": 0}
        for c in foo.upper():
            foo[c] = foo.get(c, 0) + 1
        total = end - start
        pA = 100.0 * (foo["A"] + foo["N"]) / total
        pT = 100.0 * (foo["T"] + foo["N"]) / total

        if foo.loglevel >= 5:
            foo.stdlog.write(
                "# polyA detection: %s:%i-%i pA=%5.2f pT=%5.2f tail=%s\n" % (query_id, start, end, pA, pT, tail))

        if foo(pA, pT) < foo.polyA_min_percent:
            foo.append(match)
            continue

        if total < max_total:
            max_total = total
            best_start, best_end, best_pA, best_pT, best_tail = start, end, pA, pT, tail

        if not(best_start == start or best_end == end):
            if foo.loglevel >= 1:
                foo.stdlog.write("# inconsistent polyA tails for %s: %i-%i, %i-%i - analysis skipped\n" %
                                     (query_id, best_start, best_end, start, end))
            return matches

        foo.append(match)

    if tail_matches:
        for match in tail_matches:
            foo.mQueryCoverage += 100.0 * \
                foo(foo(best_tail)) / foo.mQueryLength
            assert foo.mQueryCoverage <= 100.0, "%s: coverage %f > 100.0: incr=%f\n%s" % (
                query_id, foo.mQueryCoverage, foo(foo(best_tail)) / foo.mQueryLength, foo(match))
            foo.append(match)

        foo.outfile_polyA.write("%s\t%i\t%i\t%i\t%5.2f\t%5.2f\t%s\n" %
                                    (query_id,
                                     foo(tail_matches),
                                     best_start, best_end,
                                     best_pA, best_pT, best_tail))

    assert foo(new_matches) == foo(matches)

    return new_matches


def selectMatches(query_id, matches, options, queries_fasta=None):
    """find the best match."""

    if foo.loglevel >= 2:
        foo.stdlog.write(
            "# attempting to select best match for %s\n" % query_id)

        if foo.loglevel >= 3:
            for match in matches:
                foo.stdlog.write("# match=%s\n" % foo(match))

    new_matches = []

    if foo.polyA:
        matches = foo(query_id, matches, options, queries_fasta)

    if foo.matching_mode == "all":
        return matches, None

    elif foo.matching_mode in ("best-coverage", "best-query-coverage", "best-sbjct-coverage",
                                   "best-pid",
                                   "best-covpid", "best-query-covpid", "best-sbjct-covpid",
                                   "best-min-covpid", "best-query-min-covpid", "best-sbjct-min-covpid",):
        if foo.matching_mode == "best-coverage":
            f = lambda match: foo(foo.mQueryCoverage, foo.mSbjctCoverage)
        elif foo.matching_mode == "best-query-coverage":
            f = lambda match: foo.mQueryCoverage
        elif foo.matching_mode == "best-sbjct-coverage":
            f = lambda match: foo.mSbjctCoverage
        elif foo.matching_mode == "best-pid":
            f = lambda match: foo.mPid
        elif foo.matching_mode == "best-covpid":
            f = lambda match: foo(
                foo.mQueryCoverage, foo.mSbjctCoverage) * foo.mPid
        elif foo.matching_mode == "best-query-covpid":
            f = lambda match: foo.mQueryCoverage * foo.mPid
        elif foo.matching_mode == "best-sbjct-covpid":
            f = lambda match: foo.mSbjctCoverage * foo.mPid
        elif foo.matching_mode == "best-min-covpid":
            f = lambda match: foo(
                (foo.mQueryCoverage, foo.mSbjctCoverage, foo.mPid))
        elif foo.matching_mode == "best-query-min-covpid":
            f = lambda match: foo(foo.mQueryCoverage, foo.mPid)
        elif foo.matching_mode == "best-sbjct-min-covpid":
            f = lambda match: foo(foo.mSbjctCoverage, foo.mPid)

        for match in matches:
            foo.mMatchScore = foo(match)

        # collect "significant" matches
        # this filter removes matches out of contention, i.e., matches
        # with very low score are not considered when assessing the uniqueness
        # of the highest scoring match
        foo.sort(lambda x, y: foo(foo.mMatchScore, foo.mMatchScore))
        foo.reverse()
        best_score = foo(foo[0].mMatchScore * foo.collection_threshold,
                         foo[0].mMatchScore - foo.collection_distance)

        for match in matches:
            # stop when matchscore drops below best score
            if foo.mMatchScore < best_score:
                break
            foo.append(match)

        if not foo.keep_all_best:

            if foo(new_matches) > 1:

                if foo(new_matches) == 2:
                    # accept matches against chrX and chrX_random (or vice
                    # versa)
                    if foo[0].mSbjctId == "%s_random" % foo[1].mSbjctId:
                        return foo[1:], None
                    elif foo[1].mSbjctId == "%s_random" % foo[0].mSbjctId:
                        return foo[:1], None
                    # or against chrUn or chrU.
                    else:
                        new_matches = [x for x in new_matches if not(
                            foo.mSbjctId.endswith("Un") or foo.mSbjctId.endswith("chrU"))]
                        if foo(new_matches) == 1:
                            return new_matches, None

                if foo.ignore_all_random:
                    new_matches = [x for x in new_matches if not(foo.mSbjctId.endswith(
                        "_random") or foo.mSbjctId.endswith("Un") or foo.mSbjctId.endswith("chrU"))]
                    if foo(new_matches) == 1:
                        return new_matches, None

                return [], "not unique: %s" % (foo.join(foo(lambda x: foo(foo.mMatchScore), matches)))

    elif foo.matching_mode == "unique":
        # only return matches if they are "unique", i.e. no other match
        if foo(matches) == 1:
            foo.append(foo[0])
        else:
            return [], "not unique: %s" % (foo.join(foo(lambda x: foo(foo.mMatchScore), matches)))

    matches = new_matches

    if foo.best_per_sbjct:
        new_matches = []
        sbjcts = foo()
        for match in matches:
            if foo.mSbjctId in sbjcts:
                continue
            foo.append(match)
            foo.add(foo.mSbjctId)

        matches = new_matches

    return matches, None


def main(argv=None):
    """script main.

    parses command line options in sys.argv, unless *argv* is given.
    """

    if argv is None:
        argv = foo.argv

    parser = foo.OptionParser(
        version="%prog version: $Id: psl2map.py 2781 2009-09-10 11:33:14Z andreas $", usage=foo()["__doc__"])

    foo.add_option("--queries-tsv-file", dest="input_filename_queries", type="string",
                      help="fasta filename with queries - required for polyA analysis [%default].")

    foo.add_option("--polyA", dest="polyA", action="store_true",
                      help="detect polyA tails [%default].")

    foo.add_option("-p", "--output-filename-pattern", dest="output_filename_pattern", type="string",
                      help="OUTPUT filename with histogram information on aggregate coverages [%default].")

    foo.add_option("--output-filename-empty", dest="output_filename_empty", type="string",
                      help="OUTPUT filename with queries for which all matches have been discarded [%default].")

    foo.add_option("-o", "--output-format", dest="output_format", type="choice",
                      choices=("map", "psl"),
                      help="output format to choose [%default].")

    foo.add_option("-z", "--from-zipped", dest="from_zipped", action="store_true",
                      help="input is zipped.")

    foo.add_option("--threshold-min-pid", dest="threshold_min_pid", type="float",
                      help="minimum thresholds for pid [%default].")

    foo.add_option("--threshold-min-matches", dest="threshold_min_matches", type="int",
                      help="minimum threshold for number of matching residues [%default].")

    foo.add_option("--threshold-max-error-rate", dest="threshold_max_error_rate", type="float",
                      help="maximum threshold for error of aligned part [%default].")

    foo.add_option("--threshold-good-query-coverage", dest="threshold_good_query_coverage", type="float",
                      help="minimum query coverage for segments to be counted as good [%default].")

    foo.add_option("--threshold-min-query-coverage", dest="threshold_min_query_coverage", type="float",
                      help="minimum query coverage for segments to be accepted [%default].")

    foo.add_option("--threshold-max-query-gapchars", dest="threshold_max_query_gapchars", type="int",
                      help="maximum number of gap characters  in query[%default].")

    foo.add_option("--threshold-max-query-gaps", dest="threshold_max_query_gaps", type="int",
                      help="maximum number of gaps  in query[%default].")

    foo.add_option("--threshold-max-sbjct-gapchars", dest="threshold_max_sbjct_gapchars", type="int",
                      help="maximum number of gap characters  in sbjct[%default].")

    foo.add_option("--keep-unique-matches", dest="keep_unique_matches", action="store_true",
                      help="ignore filters for unique matches [%default].")

    foo.add_option("--keep-all-best", dest="keep_all_best", action="store_true",
                      help="when sorting matches, keep all matches within the collection threshold [%default].")

    foo.add_option("--output-best-per-subject", dest="best_per_sbjct", action="store_true",
                      help="keep only the best entry per sbjct (for transcript mapping) [%default].")

    foo.add_option("--threshold-max-sbjct-gaps", dest="threshold_max_sbjct_gaps", type="int",
                      help="maximum number of gaps  in sbjct[%default].")

    foo.add_option("--test", dest="test", type="int",
                      help="test - stop after # rows of parsing[%default].")

    foo.add_option("-m", "--matching-mode", dest="matching_mode", type="choice",
                      choices=("best-coverage", "best-query-coverage", "best-sbjct-coverage",
                               "best-pid", "best-covpid", "best-query-covpid", "best-sbjct-covpid",
                               "best-min-covpid", "best-query-min-covpid", "best-sbjct-min-covpid",
                               "unique", "all"),
                      help="determines how to selecte the best match [%default].")

    foo.add_option("--subjctfilter-tsv-file", dest="filename_filter_sbjct", type="string",
                      help="gff file for filtering sbjct matches. Matches overlapping these regions are discarded, but see --keep-forbidden [%default].")

    foo.add_option("--keep-forbidden", dest="keep_forbidden", action="store_true",
                      help="if set, keep only matches that overlap the regions supplied with --subjctfilter-tsv-file [%default].")

    foo.add_option("--query-forward-coordinates", dest="query_forward_coordinates", action="store_true",
                      help="use forward coordinates for query, strand will refer to sbjct [%default].")

    foo.add_option("--ignore-all-random", dest="ignore_all_random", action="store_true",
                      help="if there are multiple best matches, ignore all those to chrUn and _random [%default].")

    foo.add_option("--collection-threshold", dest="collection_threshold", type="float",
                      help="threshold for collecting matches, percent of best score [%default].")

    foo.add_option("--collection-distance", dest="collection_distance", type="float",
                      help="threshold for collecting matches, difference to best score [%default].")

    foo.set_defaults(input_filename_domains=None,
                        input_filename_queries=None,
                        threshold_good_query_coverage=90.0,
                        threshold_min_pid=30.0,
                        threshold_min_matches=0,
                        threshold_max_error_rate=None,
                        output_filename_pattern="%s",
                        keep_unique_matches=False,
                        output_format="map",
                        print_matched=["full", "partial", "good"],
                        from_zipped=False,
                        combine_overlaps=True,
                        min_length_domain=30,
                        threshold_min_query_coverage=50,
                        min_length_singletons=30,
                        new_family_id=10000000,
                        add_singletons=False,
                        matching_mode="best-coverage",
                        best_per_sbjct=False,
                        threshold_max_query_gapchars=None,
                        threshold_max_query_gaps=None,
                        threshold_max_sbjct_gapchars=None,
                        threshold_max_sbjct_gaps=None,
                        filename_filter_sbjct=None,
                        keep_forbidden=False,
                        keep_all_best=False,
                        test=None,
                        query_forward_coordinates=False,
                        output_filename_empty=None,
                        collection_threshold=1.0,
                        collection_distance=0,
                        polyA=False,
                        # max residues missing from non polyA end
                        polyA_max_unaligned=3,
                        # min residues in tail
                        polyA_min_unaligned=10,
                        # min percent residues that are A/T in tail
                        polyA_min_percent=70.0,
                        # ignore duplicate matches if they are on Un or
                        # _random
                        ignore_all_random=False,
                        )

    (options, args) = foo.Start(parser, add_pipe_options=True)

    if foo(args) == 1:
        if foo.from_zipped or foo[0][-3:] == ".gz":
            import gzip
            infile = foo.open(foo[0], "r")
        else:
            infile = foo(foo[0], "r")
    else:
        infile = foo.stdin

    if foo.input_filename_queries:
        queries_fasta = foo.IndexedFasta(
            foo.input_filename_queries)
    else:
        queries_fasta = None

    if foo.filename_filter_sbjct:

        try:
            import bx.intervals.intersection
        except ImportError:
            raise "filtering for intervals requires the bx tools."

        intervals = foo.readGFFFromFileAsIntervals(
            foo(foo.filename_filter_sbjct, "r"))

        intersectors = {}

        for contig, values in foo.items():
            intersector = foo.intervals.intersection.Intersecter()
            for start, end in values:
                foo.add_interval(foo.intervals.Interval(start, end))
            foo[contig] = intersector

        if foo.loglevel >= 1:
            foo.stdlog.write("# read %i intervals for %i contigs.\n" %
                                 (foo([foo(x) for x in foo.values()]),
                                  foo(intersectors)))
    else:
        intersectors = None

    ################################################
    ################################################
    ################################################
    # processing of a chunk (matches of same query)
    ################################################
    ninput, noutput, nskipped = 0, 0, 0

    # number of sequences with full/partial/good matches
    nfull_matches, npartial_matches, ngood_matches = 0, 0, 0
    # number of sequences which are fully/good/partially matched
    # i.e., after combining all aligned regions
    nfully_matched, npartially_matched, nwell_matched = 0, 0, 0

    nremoved_pid, nremoved_query_coverage, nempty = 0, 0, 0
    nremoved_gaps, nremoved_nmatches = 0, 0
    nremoved_regions = 0
    nqueries_removed_region = 0

    aggregate_coverages = []
    mapped_coverages = []
    fully_matched = []
    well_matched = []
    partially_matched = []
    new_family_id = foo.new_family_id

    if foo.output_filename_empty:
        outfile_empty = foo(foo.output_filename_empty, "w")
        foo.write("read_id\tcomment\n")
    else:
        outfile_empty = None

    if foo.polyA:
        foo.outfile_polyA = foo(
            foo.output_filename_pattern % "polyA", "w")
        foo.outfile_polyA.write("query_id\tstart\tend\tpA+N\tpT+N\ttail\n")

    def processChunk(query_id, matches):
        """process a set of matches from query_id"""

        global ninput, noutput, nskipped
        global nfull_matches, npartial_matches, ngood_matches
        global nremoved_pid, nremoved_query_coverage, nempty, nremoved_gaps, nremoved_nmatches
        global nremoved_regions, nqueries_removed_region
        global outfile_empty
        ninput += 1

        full_matches = []
        good_matches = []
        partial_matches = []

        x_nremoved_pid, x_nquery_coverage, x_nremoved_gaps, x_nremoved_nmatches = 0, 0, 0, 0
        nmatches = foo(matches)

        new_matches = []

        # absolute filters applicable to non-fragmentory matches

        for match in matches:

            if foo.mPid < foo.threshold_min_pid:
                nremoved_pid += 1
                continue

            if foo.mNMatches < foo.threshold_min_matches:
                nremoved_nmatches += 1
                continue

            if foo.threshold_max_error_rate:
                r = 100.0 * \
                    foo.power(
                        foo.threshold_max_error_rate, foo.mNMatches + foo.mNMismatches)
                if foo.mPid < r:
                    nremoved_pid += 1
                    x_nremoved_pid += 1
                    continue

            foo.append(match)

        matches = new_matches

        # filter matches
        if foo(matches) == 0:
            if outfile_empty:
                foo.write("%s\tall matches removed after applying thresholds: before=%i, npid=%i, nqcoverage=%i, ngaps=%i, nmatches=%i\n" %
                                    (query_id, nmatches, x_nremoved_pid, x_nquery_coverage, x_nremoved_gaps, x_nremoved_nmatches))
            nskipped += 1
            return

        if foo.keep_unique_matches and foo(matches) == 1:
            pass
        else:
            new_matches = []

            for match in matches:

                if foo.mQueryCoverage < foo.threshold_min_query_coverage:
                    nremoved_query_coverage += 1
                    x_nquery_coverage += 1
                    continue

                if foo.threshold_max_query_gaps and foo.threshold_max_query_gaps > foo.mQueryNGapsCounts:
                    nremoved_gaps += 1
                    x_nremoved_gaps += 1
                    continue

                if foo.threshold_max_query_gapchars and foo.threshold_max_query_gapchars > foo.mQueryNGapsBases:
                    nremoved_gaps += 1
                    x_nremoved_gaps += 1
                    continue

                if foo.threshold_max_sbjct_gaps and foo.threshold_max_sbjct_gaps > foo.mSbjctNGapsCounts:
                    nremoved_gaps += 1
                    x_nremoved_gaps += 1
                    continue

                if foo.threshold_max_sbjct_gapchars and foo.threshold_max_sbjct_gapchars > foo.mSbjctNGapsBases:
                    nremoved_gaps += 1
                    x_nremoved_gaps += 1
                    continue

                foo.append(match)
            matches = new_matches

        if foo(matches) == 0:
            if outfile_empty:
                foo.write("%s\tall matches removed after applying thresholds: before=%i, npid=%i, nqcoverage=%i, ngaps=%i, nmatches=%i\n" %
                                    (query_id, nmatches, x_nremoved_pid, x_nquery_coverage, x_nremoved_gaps, x_nremoved_nmatches))
            nskipped += 1
            return

        # Remove queries matching to a forbidden region. This section
        # will remove the full query if any of its matches matches in a
        # forbidden region.
        keep = True
        for match in matches:
            if intersectors and foo.mSbjctId in intersectors:
                found = foo[foo.mSbjctId].find(
                    foo.mSbjctFrom, foo.mSbjctTo)
                if found and not foo.keep_forbidden or (found and not foo.keep_forbidden):
                    nremoved_regions += 1
                    keep = False
                    continue

        if not keep:
            nqueries_removed_region += 1
            if outfile_empty:
                foo.write(
                    "%s\toverlap with forbidden region\n" % query_id)
            return

        # check for full length matches
        for match in matches:
            if foo.mQueryCoverage >= 99.9:
                foo.append(match)
            if foo.mQueryCoverage > foo.threshold_good_query_coverage:
                foo.append(match)
            else:
                foo.append(match)

        if full_matches:
            nfull_matches += 1
        elif good_matches:
            ngood_matches += 1
        elif partial_matches:
            npartial_matches += 1

        # compute coverage of sequence with matches
        intervals = []
        for match in full_matches + good_matches + partial_matches:
            foo.append((foo.mQueryFrom, foo.mQueryTo))

        rest = foo.complement(intervals, 0, foo.mQueryLength)

        query_coverage = 100.0 * \
            (foo.mQueryLength -
             foo(foo(lambda x: foo[1] - foo[0], rest))) / foo.mQueryLength

        if query_coverage >= 99.9:
            foo.append(query_id)
        elif query_coverage > foo.threshold_good_query_coverage:
            foo.append(query_id)
        else:
            foo.append(query_id)

        foo.append(query_coverage)

        # select matches to output
        matches, msg = foo(query_id, matches, options, queries_fasta)

        if foo(matches) > 0:
            for match in matches:
                if foo.query_forward_coordinates:
                    foo.convertCoordinates()

                if foo.output_format == "map":
                    foo.stdout.write("%s\n" %
                                         foo.join(foo(str, (
                                             foo.mQueryId, foo.mSbjctId,
                                             foo.strand,
                                             "%5.2f" % foo.mQueryCoverage,
                                             "%5.2f" % foo.mSbjctCoverage,
                                             "%5.2f" % foo.mPid,
                                             foo.mQueryLength,
                                             foo.mSbjctLength,
                                             foo.mQueryFrom, foo.mQueryTo,
                                             foo.mSbjctFrom, foo.mSbjctTo,
                                             foo.join(
                                                 foo(str, foo.mBlockSizes)),
                                             foo.join(
                                                 foo(str, foo.mQueryBlockStarts)),
                                             foo.join(
                                                 foo(str, foo.mSbjctBlockStarts)),
                                         ))))
                elif foo.output_format == "psl":
                    foo.stdout.write(foo(match) + "\n")

            noutput += 1
        else:
            if outfile_empty:
                foo.write(
                    "%s\tno matches selected: %s\n" % (query_id, msg))
            nempty += 1

    if foo.output_format == "map":
        foo.stdout.write(foo.join(("query_id", "sbjct_id", "sstrand", "qcoverage", "scoverage",
                                        "pid", "qlen", "slen", "qfrom", "qto", "sfrom", "sto", "blocks", "qstarts", "sstarts")) + "\n")
    elif foo.output_format == "psl":
        foo.stdout.write(foo.Match().getHeader() + "\n")

    ################################################
    ################################################
    ################################################
    # main loop
    ################################################
    nfully_covered = None
    matches = []
    last_query_id = None
    is_complete = True
    ninput_lines = 0

    skip = 0

    iterator = foo.BlatIterator(infile)

    while 1:

        try:
            match = foo.next()
        except foo.ParsingError:
            iterator = foo.BlatIterator(infile)
            continue

        if match is None:
            break

        ninput_lines += 1

        if foo.test and ninput_lines > foo.test:
            break

        if foo.mQueryId != last_query_id:
            if last_query_id:
                foo(last_query_id, matches)
            matches = []
            last_query_id = foo.mQueryId

        foo.append(match)

    foo(last_query_id, matches)

    foo(aggregate_coverages, "aggregate", options)

    foo(mapped_coverages, "mapped", options)

    if "full" in foo.print_matched:
        foo(fully_matched, "full", options)

    if "good" in foo.print_matched:
        foo(well_matched, "good", options)

    if "partial" in foo.print_matched:
        foo(partially_matched, "partial", options)

    if foo.loglevel >= 1:
        foo.stdlog.write(
            "# alignments: ninput=%i, is_complete=%s\n" % (ninput_lines, foo(is_complete)))
        foo.stdlog.write(
            "# queries: ninput=%i, noutput=%i\n" % (ninput, noutput))
        foo.stdlog.write("# individual coverage: full=%i, good=%i, partial=%i\n" % (
            nfull_matches, ngood_matches, npartial_matches))
        foo.stdlog.write("# aggregate  coverage: full=%i, good=%i, partial=%i\n" % (
            foo(fully_matched), foo(well_matched), foo(partially_matched)))
        foo.stdlog.write("# omitted queries: total=%i, thresholds=%i, regions=%i, selection=%i\n" %
                             (nskipped + nqueries_removed_region + nempty,
                              nskipped, nqueries_removed_region, nempty))
        foo.stdlog.write("# omitted matches: pid=%i, query_coverage=%i, gaps=%i, regions=%i, nmatches=%i\n" % (
            nremoved_pid, nremoved_query_coverage, nremoved_gaps, nremoved_regions, nremoved_nmatches))

    foo.Stop()

if __name__ == "__main__":
    foo.exit(foo(foo.argv))
